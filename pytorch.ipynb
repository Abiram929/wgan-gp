{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as tf\n",
    "from PIL import Image\n",
    "import torch.autograd as autograd\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dir_path, transform=None):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        self.ids=50\n",
    "        self.data_path = dir_path\n",
    "        self.file_names = [f for f in os.listdir(self.data_path)\n",
    "                      if f.endswith('.jpg')]\n",
    "        self.file_dict = dict()\n",
    "        for f_name in self.file_names:\n",
    "            fields = f_name.split('.')[0].split('_')\n",
    "            identity = fields[0]\n",
    "            head_pose = fields[2]\n",
    "            side = fields[-1]\n",
    "            key = '_'.join([identity, head_pose, side])\n",
    "            if key not in self.file_dict.keys():\n",
    "                self.file_dict[key] = []\n",
    "                self.file_dict[key].append(f_name)\n",
    "            else:\n",
    "                self.file_dict[key].append(f_name)\n",
    "        self.train_images = []\n",
    "        self.train_angles_r = []\n",
    "        self.train_labels = []\n",
    "        self.train_images_t = []\n",
    "        self.train_angles_g = []\n",
    "\n",
    "        self.test_images = []\n",
    "        self.test_angles_r = []\n",
    "        self.test_labels = []\n",
    "        self.test_images_t = []\n",
    "        self.test_angles_g = []\n",
    "        self.preprocess()\n",
    "    def preprocess(self):\n",
    "\n",
    "        for key in self.file_dict.keys():\n",
    "\n",
    "            if len(self.file_dict[key]) == 1:\n",
    "                continue\n",
    "\n",
    "            idx = int(key.split('_')[0])\n",
    "            flip = 1\n",
    "            if key.split('_')[-1] == 'R':\n",
    "                flip = -1\n",
    "\n",
    "            for f_r in self.file_dict[key]:\n",
    "\n",
    "                file_path = os.path.join(self.data_path, f_r)\n",
    "\n",
    "                h_angle_r = flip * float(\n",
    "                    f_r.split('_')[-2].split('H')[0]) / 15.0\n",
    "                    \n",
    "                v_angle_r = float(\n",
    "                    f_r.split('_')[-3].split('V')[0]) / 10.0\n",
    "                    \n",
    "\n",
    "                for f_g in self.file_dict[key]:\n",
    "\n",
    "                    file_path_t = os.path.join(self.data_path, f_g)\n",
    "\n",
    "                    h_angle_g = flip * float(\n",
    "                        f_g.split('_')[-2].split('H')[0]) / 15.0\n",
    "                        \n",
    "                    v_angle_g = float(\n",
    "                        f_g.split('_')[-3].split('V')[0]) / 10.0\n",
    "                        \n",
    "\n",
    "                    if idx <= self.ids:\n",
    "                        self.train_images.append(file_path)\n",
    "                        self.train_angles_r.append([h_angle_r, v_angle_r])\n",
    "                        self.train_labels.append(idx - 1)\n",
    "                        self.train_images_t.append(file_path_t)\n",
    "                        self.train_angles_g.append([h_angle_g, v_angle_g])\n",
    "                    else:\n",
    "                        self.test_images.append(file_path)\n",
    "                        self.test_angles_r.append([h_angle_r, v_angle_r])\n",
    "                        self.test_labels.append(idx - 1)\n",
    "                        self.test_images_t.append(file_path_t)\n",
    "                        self.test_angles_g.append([h_angle_g, v_angle_g])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            self.transform(Image.open(self.train_images[index])),\n",
    "                torch.tensor(self.train_angles_r[index]),\n",
    "                self.train_labels[index],\n",
    "            self.transform(Image.open(self.train_images_t[index])),\n",
    "                torch.tensor(self.train_angles_g[index]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_images)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=tf.Compose([tf.ToTensor(),tf.Resize((64,64),antialias=True)])\n",
    "dataset=MyDataset(dir_path='/home/user/Downloads/dataset/0P',transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 64, 64]) torch.Size([32, 2]) torch.Size([32]) torch.Size([32, 3, 64, 64]) torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "imgs_r,angles_r,labels,imgs_g,angles_g=next(iter(train_loader))\n",
    "print(imgs_r.shape,angles_r.shape,labels.shape,imgs_g.shape,angles_g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_net import Generator,Discriminator,Generator2\n",
    "from net import NetD\n",
    "generator=Generator2()\n",
    "discriminator=Discriminator()\n",
    "generator=generator.to(device)\n",
    "discriminator=discriminator.to(device)\n",
    "LR = 5e-5\n",
    "beta1=0.5\n",
    "beta2=0.999\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), LR,betas=(beta1, beta2))\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), LR,betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/pytorch2/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/envs/pytorch2/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from loss_network import LossNetwork\n",
    "loss_network=LossNetwork()\n",
    "loss_network=loss_network.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import content_style_loss,adv_loss,gaze_loss_d,gaze_loss_g,reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_step(generator,discriminator,loss_network,imgs_r,imgs_t,angles_r,angles_g):\n",
    "    optimizer_g.zero_grad()\n",
    "    generator.train()\n",
    "    discriminator.eval()\n",
    "    x_g=generator(imgs_r,angles_g)\n",
    "    x_recon=generator(x_g,angles_r)\n",
    "    loss_adv=-adv_loss(discriminator,imgs_r,x_g)\n",
    "    loss2=content_style_loss(loss_network,x_g,imgs_t)\n",
    "    loss_p=loss2[0]+loss2[1]\n",
    "    loss_gg=gaze_loss_g(discriminator,x_g,angles_g)\n",
    "    loss_recon=reconstruction_loss(generator,imgs_r,x_recon)\n",
    "    loss=loss_adv+100*loss_p+5*loss_gg+50*loss_recon\n",
    "    loss.backward()\n",
    "    optimizer_g.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_step(generator,discriminator,imgs_r,imgs_t,angles_r,angles_g):\n",
    "    optimizer_d.zero_grad()\n",
    "    generator.eval()\n",
    "    discriminator.train()\n",
    "    x_g=generator(imgs_r,angles_g)\n",
    "    loss1=adv_loss(discriminator,imgs_r,x_g)\n",
    "    loss2=gaze_loss_d(discriminator,imgs_r,angles_r)\n",
    "    loss=loss1+5*loss2\n",
    "    loss.backward()\n",
    "    optimizer_d.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "def recover_image(img):\n",
    "    img=img.cpu().numpy().transpose(0, 2, 3, 1)*255\n",
    "    return img.astype(np.uint8)\n",
    "def save_debug_image(tensor_orig, tensor_transformed, filename):\n",
    "    assert tensor_orig.size() == tensor_transformed.size()\n",
    "    result = Image.fromarray(recover_image(tensor_transformed)[0])\n",
    "    orig = Image.fromarray(recover_image(tensor_orig)[0])\n",
    "    new_im = Image.new('RGB', (result.size[0] * 2 + 5, result.size[1]))\n",
    "    new_im.paste(orig, (0,0))\n",
    "    new_im.paste(result, (result.size[0] + 5,0))\n",
    "    new_im.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/user/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.37819546461105347 3.63523530960083\n",
      "-0.14207199215888977 2.755223512649536\n",
      "-0.3136076331138611 2.468665361404419\n",
      "-0.31502389907836914 2.308568000793457\n",
      "-0.2929360270500183 1.9274978637695312\n",
      "-0.2526772916316986 1.8445425033569336\n",
      "-0.2699839174747467 2.0032482147216797\n",
      "-0.18012413382530212 1.8072530031204224\n",
      "-0.23193061351776123 1.5845448970794678\n",
      "-0.2523336112499237 1.643057107925415\n",
      "-0.21316324174404144 1.5631529092788696\n",
      "-0.2323027402162552 1.5290582180023193\n",
      "-0.16548000276088715 1.3630588054656982\n",
      "-0.1660952866077423 1.418100357055664\n",
      "-0.24043692648410797 1.3424537181854248\n",
      "-0.18404263257980347 1.1349809169769287\n",
      "-0.2171579748392105 1.1743931770324707\n",
      "-0.15232275426387787 1.3872486352920532\n",
      "-0.1781960129737854 1.342564582824707\n",
      "-0.15805666148662567 1.1742628812789917\n",
      "-0.1667582243680954 1.201562762260437\n",
      "-0.19394522905349731 1.0438423156738281\n",
      "-0.19095367193222046 1.2095228433609009\n",
      "-0.14213384687900543 1.393747091293335\n",
      "-0.2327486127614975 1.237069845199585\n",
      "-0.17708763480186462 1.0451375246047974\n",
      "-0.19971279799938202 1.2235872745513916\n",
      "-0.1917891949415207 0.9924639463424683\n",
      "-0.14097696542739868 1.040691614151001\n",
      "-0.20602037012577057 1.072937250137329\n",
      "-0.24506732821464539 1.1604745388031006\n",
      "-0.19077712297439575 1.1543811559677124\n",
      "-0.2928655445575714 1.0348349809646606\n",
      "-0.1701425164937973 0.9692870378494263\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs=300\n",
    "for epoch in range(epochs):\n",
    "    count=0\n",
    "    a=torch.tile(torch.tensor([0.,0.]),[32,1])\n",
    "    b=torch.tile(torch.tensor([-1.,-1.]),[32,1])\n",
    "    c=torch.tile(torch.tensor([1.,1.]),[32,1])\n",
    "    #print(a.shape)\n",
    "    #y=generator(imgs_r.to(device),a.to(device))\n",
    "    for imgs_r, angles_r, labels, imgs_t, angles_g in train_loader:\n",
    "        count+=1\n",
    "        imgs_r=imgs_r.to(device)\n",
    "        imgs_t=imgs_t.to(device)\n",
    "        angles_r=angles_r.to(device)\n",
    "        angles_g=angles_g.to(device)\n",
    "        l_d=discriminator_step(generator,discriminator,imgs_r,imgs_t,angles_r,angles_g)\n",
    "        if count%5==0:\n",
    "            l_g=generator_step(generator,discriminator,loss_network,imgs_r,imgs_t,angles_r,angles_g)\n",
    "        if count%1000==0:\n",
    "            #a=torch.tile(torch.tensor([0.,0.]),[32,1])\n",
    "            ya=generator(imgs_r,a.to(device))\n",
    "            yb=generator(imgs_r,b.to(device))\n",
    "            yc=generator(imgs_r,c.to(device))\n",
    "            save_debug_image(imgs_r, ya.detach(), \"./debug/{}_{}_a.png\".format(epoch,count))\n",
    "            save_debug_image(imgs_r, yb.detach(), \"./debug/{}_{}_b.png\".format(epoch,count))\n",
    "            save_debug_image(imgs_r, yc.detach(), \"./debug/{}_{}_c.png\".format(epoch,count))\n",
    "    print(l_d,l_g)\n",
    "\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tile(torch.tensor([0.,0.]),[32,1])\n",
    "print(a.size(),angles_r.size())\n",
    "y=generator(imgs_r.to(device),angles_r.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
