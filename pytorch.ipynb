{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as tf\n",
    "from PIL import Image\n",
    "import torch.autograd as autograd\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dir_path, transform=None):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        self.ids=50\n",
    "        self.data_path = dir_path\n",
    "        self.file_names = [f for f in os.listdir(self.data_path)\n",
    "                      if f.endswith('.jpg')]\n",
    "        self.file_dict = dict()\n",
    "        for f_name in self.file_names:\n",
    "            fields = f_name.split('.')[0].split('_')\n",
    "            identity = fields[0]\n",
    "            head_pose = fields[2]\n",
    "            side = fields[-1]\n",
    "            key = '_'.join([identity, head_pose, side])\n",
    "            if key not in self.file_dict.keys():\n",
    "                self.file_dict[key] = []\n",
    "                self.file_dict[key].append(f_name)\n",
    "            else:\n",
    "                self.file_dict[key].append(f_name)\n",
    "        self.train_images = []\n",
    "        self.train_angles_r = []\n",
    "        self.train_labels = []\n",
    "        self.train_images_t = []\n",
    "        self.train_angles_g = []\n",
    "\n",
    "        self.test_images = []\n",
    "        self.test_angles_r = []\n",
    "        self.test_labels = []\n",
    "        self.test_images_t = []\n",
    "        self.test_angles_g = []\n",
    "        self.preprocess()\n",
    "    def preprocess(self):\n",
    "\n",
    "        for key in self.file_dict.keys():\n",
    "\n",
    "            if len(self.file_dict[key]) == 1:\n",
    "                continue\n",
    "\n",
    "            idx = int(key.split('_')[0])\n",
    "            flip = 1\n",
    "            if key.split('_')[-1] == 'R':\n",
    "                flip = -1\n",
    "\n",
    "            for f_r in self.file_dict[key]:\n",
    "\n",
    "                file_path = os.path.join(self.data_path, f_r)\n",
    "\n",
    "                h_angle_r = flip * float(\n",
    "                    f_r.split('_')[-2].split('H')[0]) / 15.0\n",
    "                    \n",
    "                v_angle_r = float(\n",
    "                    f_r.split('_')[-3].split('V')[0]) / 10.0\n",
    "                    \n",
    "\n",
    "                for f_g in self.file_dict[key]:\n",
    "\n",
    "                    file_path_t = os.path.join(self.data_path, f_g)\n",
    "\n",
    "                    h_angle_g = flip * float(\n",
    "                        f_g.split('_')[-2].split('H')[0]) / 15.0\n",
    "                        \n",
    "                    v_angle_g = float(\n",
    "                        f_g.split('_')[-3].split('V')[0]) / 10.0\n",
    "                        \n",
    "\n",
    "                    if idx <= self.ids:\n",
    "                        self.train_images.append(file_path)\n",
    "                        self.train_angles_r.append([h_angle_r, v_angle_r])\n",
    "                        self.train_labels.append(idx - 1)\n",
    "                        self.train_images_t.append(file_path_t)\n",
    "                        self.train_angles_g.append([h_angle_g, v_angle_g])\n",
    "                    else:\n",
    "                        self.test_images.append(file_path)\n",
    "                        self.test_angles_r.append([h_angle_r, v_angle_r])\n",
    "                        self.test_labels.append(idx - 1)\n",
    "                        self.test_images_t.append(file_path_t)\n",
    "                        self.test_angles_g.append([h_angle_g, v_angle_g])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            self.transform(Image.open(self.train_images[index])),\n",
    "                torch.tensor(self.train_angles_r[index]),\n",
    "                self.train_labels[index],\n",
    "            self.transform(Image.open(self.train_images_t[index])),\n",
    "                torch.tensor(self.train_angles_g[index]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_images)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=tf.Compose([tf.ToTensor(),tf.Resize((64,64),antialias=True)])\n",
    "# dataset=MyDataset(dir_path='/home/user/Downloads/dataset/0P',transform=transform)\n",
    "dataset=MyDataset(dir_path='c:\\\\Users\\\\hikma\\\\Downloads/dataset/0P',transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 64, 64]) torch.Size([32, 2]) torch.Size([32]) torch.Size([32, 3, 64, 64]) torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "imgs_r,angles_r,labels,imgs_g,angles_g=next(iter(train_loader))\n",
    "print(imgs_r.shape,angles_r.shape,labels.shape,imgs_g.shape,angles_g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_net import Generator,Discriminator,Generator2\n",
    "from net import NetD\n",
    "generator=Generator2()\n",
    "discriminator=Discriminator()\n",
    "generator=generator.to(device)\n",
    "discriminator=discriminator.to(device)\n",
    "LR = 5e-5\n",
    "beta1=0.5\n",
    "beta2=0.999\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), LR,betas=(beta1, beta2))\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), LR,betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hikma\\anaconda3\\envs\\jupyter_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hikma\\anaconda3\\envs\\jupyter_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from loss_network import LossNetwork\n",
    "loss_network=LossNetwork()\n",
    "loss_network=loss_network.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import content_style_loss,adv_loss_d,adv_loss_g,gaze_loss_d,gaze_loss_g,reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_step(generator,discriminator,loss_network,imgs_r,imgs_t,angles_r,angles_g):\n",
    "    optimizer_g.zero_grad()\n",
    "    generator.train()\n",
    "    discriminator.eval()\n",
    "    x_g=generator(imgs_r,angles_g)\n",
    "    x_recon=generator(x_g,angles_r)\n",
    "    loss_adv=adv_loss_g(discriminator,imgs_r,x_g)\n",
    "    loss2=content_style_loss(loss_network,x_g,imgs_t)\n",
    "    loss_p=loss2[0]+loss2[1]\n",
    "    loss_gg=gaze_loss_g(discriminator,x_g,angles_g)\n",
    "    loss_recon=reconstruction_loss(generator,imgs_r,x_recon)\n",
    "    loss=loss_adv+100*loss_p+5*loss_gg+50*loss_recon\n",
    "    loss.backward()\n",
    "    optimizer_g.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_step(generator,discriminator,imgs_r,imgs_t,angles_r,angles_g):\n",
    "    optimizer_d.zero_grad()\n",
    "    generator.eval()\n",
    "    discriminator.train()\n",
    "    x_g=generator(imgs_r,angles_g)\n",
    "    loss1=adv_loss_d(discriminator,imgs_r,x_g)\n",
    "    loss2=gaze_loss_d(discriminator,imgs_r,angles_r)\n",
    "    loss=loss1+5*loss2\n",
    "    loss.backward()\n",
    "    optimizer_d.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "def recover_image(img):\n",
    "    img=img.cpu().numpy().transpose(0, 2, 3, 1)*255\n",
    "    return img.astype(np.uint8)\n",
    "def save_debug_image(tensor_orig, tensor_transformed, filename):\n",
    "    assert tensor_orig.size() == tensor_transformed.size()\n",
    "    result = Image.fromarray(recover_image(tensor_transformed)[0])\n",
    "    orig = Image.fromarray(recover_image(tensor_orig)[0])\n",
    "    new_im = Image.new('RGB', (result.size[0] * 2 + 5, result.size[1]))\n",
    "    new_im.paste(orig, (0,0))\n",
    "    new_im.paste(result, (result.size[0] + 5,0))\n",
    "    new_im.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file -p already exists.\n",
      "Error occurred while processing: -p.\n",
      "A subdirectory or file debug already exists.\n",
      "Error occurred while processing: debug.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.14090707898139954 3.1814870834350586\n",
      "-0.3075118660926819 2.5949172973632812\n",
      "-0.31422021985054016 2.496328353881836\n",
      "-0.28986045718193054 1.9234073162078857\n",
      "-0.20188328623771667 1.5243966579437256\n",
      "-0.1502615213394165 1.7172067165374756\n",
      "-0.24869534373283386 1.580897331237793\n",
      "-0.21149319410324097 1.5009021759033203\n",
      "-0.34318530559539795 1.5165023803710938\n",
      "-0.15337936580181122 1.3503401279449463\n",
      "-0.21691101789474487 1.188339114189148\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m angles_r\u001b[39m=\u001b[39mangles_r\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m angles_g\u001b[39m=\u001b[39mangles_g\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 15\u001b[0m l_d\u001b[39m=\u001b[39mdiscriminator_step(generator,discriminator,imgs_r,imgs_t,angles_r,angles_g)\n\u001b[0;32m     16\u001b[0m \u001b[39mif\u001b[39;00m count\u001b[39m%\u001b[39m\u001b[39m5\u001b[39m\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[0;32m     17\u001b[0m     l_g\u001b[39m=\u001b[39mgenerator_step(generator,discriminator,loss_network,imgs_r,imgs_t,angles_r,angles_g)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "epochs=300\n",
    "for epoch in range(epochs):\n",
    "    count=0\n",
    "    a=torch.tile(torch.tensor([0.,0.]),[32,1])\n",
    "    b=torch.tile(torch.tensor([-1.,-1.]),[32,1])\n",
    "    c=torch.tile(torch.tensor([1.,1.]),[32,1])\n",
    "    #print(a.shape)\n",
    "    #y=generator(imgs_r.to(device),a.to(device))\n",
    "    for imgs_r, angles_r, labels, imgs_t, angles_g in train_loader:\n",
    "        count+=1\n",
    "        imgs_r=imgs_r.to(device)\n",
    "        imgs_t=imgs_t.to(device)\n",
    "        angles_r=angles_r.to(device)\n",
    "        angles_g=angles_g.to(device)\n",
    "        l_d=discriminator_step(generator,discriminator,imgs_r,imgs_t,angles_r,angles_g)\n",
    "        if count%5==0:\n",
    "            l_g=generator_step(generator,discriminator,loss_network,imgs_r,imgs_t,angles_r,angles_g)\n",
    "        if count%1000==0:\n",
    "            #a=torch.tile(torch.tensor([0.,0.]),[32,1])\n",
    "            ya=generator(imgs_r,a.to(device))\n",
    "            yb=generator(imgs_r,b.to(device))\n",
    "            yc=generator(imgs_r,c.to(device))\n",
    "            save_debug_image(imgs_r, ya.detach(), \"./debug/{}_{}_a.png\".format(epoch,count))\n",
    "            save_debug_image(imgs_r, yb.detach(), \"./debug/{}_{}_b.png\".format(epoch,count))\n",
    "            save_debug_image(imgs_r, yc.detach(), \"./debug/{}_{}_c.png\".format(epoch,count))\n",
    "    print(l_d,l_g)\n",
    "\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tile(torch.tensor([0.,0.]),[32,1])\n",
    "print(a.size(),angles_r.size())\n",
    "y=generator(imgs_r.to(device),angles_r.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
