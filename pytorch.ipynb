{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as tf\n",
    "from PIL import Image\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images files have the form \"ID_2m_0P_xV_yH_z.jpg\" where ID is the ID of the person, 2m is fixed, 0P means head pose of 0 degrees (only head pose used in this notebook)\n",
    "x is the vertical orientation, y is the horizontal orientation and z is either L for left or R for right eye (note that the right eye patch was flipped horizontally).\n",
    "In training the images are grouped as follows:\n",
    "For a given person and a given eye (R or L) all orientations are grouped together. One element of the data set is of the form\n",
    "imgs_r,angles_r,labels,imgs_t,angles_g where imgs_r is considered the \"real\" image with orientation angles_r, or x_r in the paper,\n",
    "imgs_t with orientation angles_g is the image of the same person with different orientation (could be the same image since we go through a double loop) and the label is the ID of the person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dir_path, transform=None):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        self.ids=50\n",
    "        self.data_path = dir_path\n",
    "        self.file_names = [f for f in os.listdir(self.data_path)\n",
    "                      if f.endswith('.jpg')]\n",
    "        self.file_dict = dict()\n",
    "        for f_name in self.file_names:\n",
    "            fields = f_name.split('.')[0].split('_')\n",
    "            identity = fields[0]\n",
    "            head_pose = fields[2]\n",
    "            side = fields[-1]\n",
    "            key = '_'.join([identity, head_pose, side])\n",
    "            if key not in self.file_dict.keys():\n",
    "                self.file_dict[key] = []\n",
    "                self.file_dict[key].append(f_name)\n",
    "            else:\n",
    "                self.file_dict[key].append(f_name)\n",
    "        self.train_images = []\n",
    "        self.train_angles_r = []\n",
    "        self.train_labels = []\n",
    "        self.train_images_t = []\n",
    "        self.train_angles_g = []\n",
    "\n",
    "        self.test_images = []\n",
    "        self.test_angles_r = []\n",
    "        self.test_labels = []\n",
    "        self.test_images_t = []\n",
    "        self.test_angles_g = []\n",
    "        self.preprocess()\n",
    "    def preprocess(self):\n",
    "\n",
    "        for key in self.file_dict.keys():\n",
    "\n",
    "            if len(self.file_dict[key]) == 1:\n",
    "                continue\n",
    "\n",
    "            idx = int(key.split('_')[0])\n",
    "            flip = 1\n",
    "            if key.split('_')[-1] == 'R':\n",
    "                flip = -1\n",
    "\n",
    "            for f_r in self.file_dict[key]:\n",
    "\n",
    "                file_path = os.path.join(self.data_path, f_r)\n",
    "\n",
    "                h_angle_r = flip * float(\n",
    "                    f_r.split('_')[-2].split('H')[0]) / 15.0\n",
    "                    \n",
    "                v_angle_r = float(\n",
    "                    f_r.split('_')[-3].split('V')[0]) / 10.0\n",
    "                    \n",
    "\n",
    "                for f_g in self.file_dict[key]:\n",
    "\n",
    "                    file_path_t = os.path.join(self.data_path, f_g)\n",
    "\n",
    "                    h_angle_g = flip * float(\n",
    "                        f_g.split('_')[-2].split('H')[0]) / 15.0\n",
    "                        \n",
    "                    v_angle_g = float(\n",
    "                        f_g.split('_')[-3].split('V')[0]) / 10.0\n",
    "                        \n",
    "\n",
    "                    if idx <= self.ids:\n",
    "                        self.train_images.append(file_path)\n",
    "                        self.train_angles_r.append([h_angle_r, v_angle_r])\n",
    "                        self.train_labels.append(idx - 1)\n",
    "                        self.train_images_t.append(file_path_t)\n",
    "                        self.train_angles_g.append([h_angle_g, v_angle_g])\n",
    "                    else:\n",
    "                        self.test_images.append(file_path)\n",
    "                        self.test_angles_r.append([h_angle_r, v_angle_r])\n",
    "                        self.test_labels.append(idx - 1)\n",
    "                        self.test_images_t.append(file_path_t)\n",
    "                        self.test_angles_g.append([h_angle_g, v_angle_g])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            self.transform(Image.open(self.train_images[index])),\n",
    "                torch.tensor(self.train_angles_r[index]),\n",
    "                self.train_labels[index],\n",
    "            self.transform(Image.open(self.train_images_t[index])),\n",
    "                torch.tensor(self.train_angles_g[index]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_images)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=tf.Compose([tf.ToTensor(),tf.Resize((64,64),antialias=True)])\n",
    "# dataset=MyDataset(dir_path='/home/user/Downloads/dataset/0P',transform=transform)\n",
    "dataset=MyDataset(dir_path='/home/user/Downloads/dataset/0P',transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 64, 64]) torch.Size([32, 2]) torch.Size([32]) torch.Size([32, 3, 64, 64]) torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "imgs_r,angles_r,labels,imgs_g,angles_g=next(iter(train_loader))\n",
    "print(imgs_r.shape,angles_r.shape,labels.shape,imgs_g.shape,angles_g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import Discriminator,Generator2\n",
    "# if os.path.isfile('discriminator.pth'):\n",
    "#     discriminator=torch.load('discriminator.pth')\n",
    "#     print('loaded discriminator')\n",
    "# else:\n",
    "#     discriminator=Discriminator()\n",
    "#     print('created discriminator')\n",
    "# if os.path.isfile('generator.pth'):\n",
    "#     generator=torch.load('generator.pth')\n",
    "#     print('loaded generator')\n",
    "# else:\n",
    "#     generator=Generator2()\n",
    "#     print('created generator')\n",
    "\n",
    "discriminator=Discriminator()\n",
    "generator=Generator2()\n",
    "generator=generator.to(device)\n",
    "discriminator=discriminator.to(device)\n",
    "LR = 5e-5\n",
    "beta1=0.5\n",
    "beta2=0.999\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), LR,betas=(beta1, beta2))\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), LR,betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/pytorch2/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/anaconda3/envs/pytorch2/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from loss_network import LossNetwork\n",
    "loss_network=LossNetwork()\n",
    "loss_network=loss_network.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import content_style_loss,adv_loss_d,adv_loss_g,gaze_loss_d,gaze_loss_g,reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_step(generator,discriminator,loss_network,imgs_r,imgs_t,angles_r,angles_g):\n",
    "    optimizer_g.zero_grad()\n",
    "    generator.train()\n",
    "    discriminator.eval()\n",
    "    x_g=generator(imgs_r,angles_g)\n",
    "    x_recon=generator(x_g,angles_r)\n",
    "    loss_adv=adv_loss_g(discriminator,imgs_r,x_g)\n",
    "    loss2=content_style_loss(loss_network,x_g,imgs_t)\n",
    "    loss_p=loss2[0]+loss2[1]\n",
    "    loss_gg=gaze_loss_g(discriminator,x_g,angles_g)\n",
    "    loss_recon=reconstruction_loss(generator,imgs_r,x_recon)\n",
    "    loss=loss_adv+100*loss_p+5*loss_gg+50*loss_recon\n",
    "    loss.backward()\n",
    "    optimizer_g.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_step(generator,discriminator,imgs_r,imgs_t,angles_r,angles_g):\n",
    "    optimizer_d.zero_grad()\n",
    "    generator.eval()\n",
    "    discriminator.train()\n",
    "    x_g=generator(imgs_r,angles_g)\n",
    "    loss1=adv_loss_d(discriminator,imgs_r,x_g)\n",
    "    loss2=gaze_loss_d(discriminator,imgs_r,angles_r)\n",
    "    loss=loss1+5*loss2\n",
    "    loss.backward()\n",
    "    optimizer_d.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "def recover_image(img):\n",
    "    img=img.cpu().numpy().transpose(0, 2, 3, 1)*255\n",
    "    return img.astype(np.uint8)\n",
    "def save_images(imgs, filename):\n",
    "    height=recover_image(imgs[0])[0].shape[0]\n",
    "    width=recover_image(imgs[0])[0].shape[1]\n",
    "    total_width=width*len(imgs)\n",
    "    \n",
    "    new_im = Image.new('RGB', (total_width+len(imgs), height))\n",
    "    for i,img in enumerate(imgs):\n",
    "        result = Image.fromarray(recover_image(img)[0])\n",
    "        new_im.paste(result, (i*width+i,0))\n",
    "    new_im.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/user/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs=300\n",
    "for epoch in range(epochs):\n",
    "    count=0\n",
    "  \n",
    "    for imgs_r, angles_r, labels, imgs_t, angles_g in train_loader:\n",
    "        count+=1\n",
    "        imgs_r=imgs_r.to(device)\n",
    "        imgs_t=imgs_t.to(device)\n",
    "        angles_r=angles_r.to(device)\n",
    "        angles_g=angles_g.to(device)\n",
    "        l_d=discriminator_step(generator,discriminator,imgs_r,imgs_t,angles_r,angles_g)\n",
    "        if count%5==0:\n",
    "            l_g=generator_step(generator,discriminator,loss_network,imgs_r,imgs_t,angles_r,angles_g)\n",
    "        if count%1000==0:\n",
    "            imgs=[imgs_r]\n",
    "            for h in [-15,-10,-5,0,5,10,15]:\n",
    "                    a=torch.tile(torch.tensor([h/15.,0.]),[32,1])\n",
    "                    a=a.to(device)\n",
    "                    y=generator(imgs_r,a)\n",
    "                    imgs.append(y.detach())\n",
    "            save_images(imgs, \"./debug/{}_{}.png\".format(epoch,count))\n",
    "    print(l_d,l_g)\n",
    "    if epoch%20==0:\n",
    "        torch.save(generator, './generator.pth')\n",
    "        torch.save(discriminator, './discriminator.pth')\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tile(torch.tensor([0.,0.]),[32,1])\n",
    "print(a.size(),angles_r.size())\n",
    "y=generator(imgs_r.to(device),angles_r.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
